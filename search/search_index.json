{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"spalah","text":"<p>Spalah is a set of python helpers to deal with PySpark dataframes, transformations, schemas etc.</p> <p>It's main feature is to simplify dealing with advanced spark schemas. Think nested structures, arrays, arrays in arrays in nested structures in arrays. Sometimes such schemas happens. Especially if the lakehouses stores ingested json dataset as is.</p> <p>And.. the word \"spalah\" means \"spark\" in Ukrainian \ud83c\uddfa\ud83c\udde6 :)</p> <p> </p> <p>Documentation: https://avolok.github.io/spalah Source Code for spalah: https://github.com/avolok/spalah</p>"},{"location":"#installation","title":"Installation","text":"<p>Use the package manager pip to install spalah.</p> <p> pip install spalah Installed </p>"},{"location":"#examples","title":"Examples","text":""},{"location":"#slicing-complex-schema-by-removing-or-nullifying-nested-elements","title":"Slicing complex schema by removing (or nullifying) nested elements","text":"<pre><code>from spalah.dataframe import slice_dataframe\n\ndf = spark.sql(\n    'SELECT 1 as ID, \"John\" AS Name, struct(\"line1\" AS Line1, \"line2\" AS Line2) AS Address'\n)\ndf.printSchema()\n\n\"\"\" output:\nroot\n |-- ID: integer (nullable = false)\n |-- Name: string (nullable = false)\n |-- Address: struct (nullable = false)\n |    |-- Line1: string (nullable = false)\n |    |-- Line2: string (nullable = false)\n\"\"\"\n\n# Create a new dataframe by cutting of root and nested attributes\ndf_result = slice_dataframe(\n    input_dataframe=df,\n    columns_to_include=[\"Name\", \"Address\"],\n    columns_to_exclude=[\"Address.Line2\"]\n)\ndf_result.printSchema()\n\n\"\"\" output:\nroot\n |-- Name: string (nullable = false)\n |-- Address: struct (nullable = false)\n |    |-- Line1: string (nullable = false)\n\"\"\"\n</code></pre> <p>Note</p> <p>Beside of nested regular structs it also supported slicing of structs in arrays, including multiple levels of nesting</p>"},{"location":"#get-list-of-flattened-elements-from-the-complex-schema","title":"Get list of flattened elements from the complex schema","text":"All elements flattened to a single dimension listUse tuples to return element name and data type <pre><code>from spalah.dataframe import flatten_schema\n\n# Pass the sample dataframe to get the list of all attributes as single dimension list\nflatten_schema(df_complex_schema.schema)\n\n\"\"\" output:\n['ID', 'Name', 'Address.Line1', 'Address.Line2']\n\"\"\"\n</code></pre> <pre><code>from spalah.dataframe import flatten_schema\n\n# Alternatively, the function can return data types of the attributes\nflatten_schema(\n    schema=df_complex_schema.schema,\n    include_datatype=True\n)\n\n\"\"\" output:\n[\n    ('ID', 'IntegerType'),\n    ('Name', 'StringType'),\n    ('Address.Line1', 'StringType'),\n    ('Address.Line2', 'StringType')\n]\n\"\"\"\n</code></pre>"},{"location":"#set-delta-table-properties-and-check-constraints","title":"Set Delta Table properties and check constraints","text":"<pre><code>from spalah.dataset import DeltaTableConfig\n\ndp = DeltaTableConfig(table_path=\"/tmp/nested_schema_dataset\")\n\ndp.properties = {\n    \"delta.logRetentionDuration\": \"interval 10 days\",\n    \"delta.deletedFileRetentionDuration\": \"interval 15 days\"\n}\ndp.check_constraints = {'id_is_not_null': 'id is not null'} \n</code></pre> <p>See more examples in examples: dataframes and examples: dataset</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the MIT license.</p>"},{"location":"examples/dataframe/","title":"Examples of use: spalah.dataframe","text":"<p>This module contains various dataframe specific functions and classes, like <code>SchemaComparer</code>, <code>script_dataframe</code>, <code>slice_dataframe</code> etc. </p>"},{"location":"examples/dataframe/#slice_dataframe","title":"slice_dataframe","text":"<p>Slice the schema of the dataframe by selecting which attributes must be included and/or excluded. The function supports also complex structures and can be helpful for cases when sensitive/PII attributes must be cut off during the data transformation: <pre><code>from spalah.dataframe import slice_dataframe\n\ndf = spark.sql(\n    'SELECT 1 as ID, \"John\" AS Name, struct(\"line1\" AS Line1, \"line2\" AS Line2) AS Address'\n)\ndf.printSchema()\n\n\"\"\" output:\nroot\n |-- ID: integer (nullable = false)\n |-- Name: string (nullable = false)\n |-- Address: struct (nullable = false)\n |    |-- Line1: string (nullable = false)\n |    |-- Line2: string (nullable = false)\n\"\"\"\n\n# Create a new dataframe by cutting of root and nested attributes\ndf_result = slice_dataframe(\n    input_dataframe=df,\n    columns_to_include=[\"Name\", \"Address\"],\n    columns_to_exclude=[\"Address.Line2\"]\n)\ndf_result.printSchema()\n\n\"\"\" output:\nroot\n |-- Name: string (nullable = false)\n |-- Address: struct (nullable = false)\n |    |-- Line1: string (nullable = false)\n\"\"\"\n</code></pre></p> <p>Alternatively, excluded columns can be nullified instead of removed: <pre><code>df_result = slice_dataframe(\n    input_dataframe=df,\n    columns_to_include=[\"Name\", \"Address\"],\n    columns_to_exclude=[\"Address.Line2\"],\n    nullify_only=True\n)\n\ndf_result.show()\n\n\"\"\" output:\n+----+----+-------------+\n|  ID|Name|      Address|\n+----+----+-------------+\n|null|John|{line1, null}|\n+----+----+-------------+\n\n\"\"\"\n</code></pre></p> <p>Beside of nested regular structs it also supported slicing of such in arrays, including multiple levels of nesting.</p> <p>Following schema example contains</p> <pre><code>root\n |-- parent_struct: struct (nullable = false)\n |    |-- struct_in_array: array (nullable = false)\n |    |    |-- element: struct (containsNull = false)\n |    |    |    |-- a: integer (nullable = false)\n |    |    |    |-- b: integer (nullable = false)\n |    |    |    |-- c_array: array (nullable = false)\n |    |    |    |    |-- element: struct (containsNull = false)\n |    |    |    |    |    |-- cc: string (nullable = false)\n |    |    |    |    |    |-- dd: string (nullable = false)\n |    |    |    |    |    |-- g: array (nullable = false)\n |    |    |    |    |    |    |-- element: struct (containsNull = false)\n |    |    |    |    |    |    |    |-- ee: string (nullable = false)\n |    |    |    |    |    |    |    |-- ff: integer (nullable = false)\n |    |    |    |    |    |-- h: struct (nullable = false)\n |    |    |    |    |    |    |-- zz: string (nullable = false)\n |    |    |    |    |    |    |-- yy: integer (nullable = false)\n |    |-- storeid: integer (nullable = false)\n</code></pre> <p>Following snippet will slice the array to contain only the subset of schema, including multiple levels of arrays:</p> <pre><code>df_out = project_dataframe_schema(\n    input_dataframe=df_h,\n\n    columns_to_include=[\"parent_struct.struct_in_array.c_array.g\"],\n    nullify_only=False,\n    debug=False,\n    )\ndf_out.printSchema()\n\nroot\n |-- parent_struct: struct (nullable = false)\n |    |-- struct_in_array: array (nullable = false)\n |    |    |-- element: struct (containsNull = false)\n |    |    |    |-- c_array: array (nullable = false)\n |    |    |    |    |-- element: struct (containsNull = false)\n |    |    |    |    |    |-- g: array (nullable = false)\n |    |    |    |    |    |    |-- element: struct (containsNull = false)\n |    |    |    |    |    |    |    |-- ee: string (nullable = false)\n |    |    |    |    |    |    |    |-- ff: integer (nullable = false)\n</code></pre> <p>Another example inverts the output by excluding a child array element <code>parent_struct.struct_in_array.c_array</code>:</p> <pre><code>df_out = project_dataframe_schema(\n    input_dataframe=df_h,\n\n    columns_to_exclude=[\"parent_struct.struct_in_array.c_array\"],\n    nullify_only=False,\n    debug=False,\n    )\ndf_out.printSchema()\n\n\nroot\n |-- parent_struct: struct (nullable = false)\n |    |-- struct_in_array: array (nullable = false)\n |    |    |-- element: struct (containsNull = false)\n |    |    |    |-- a: integer (nullable = false)\n |    |    |    |-- b: integer (nullable = false)\n |    |-- storeid: integer (nullable = false)\n</code></pre>"},{"location":"examples/dataframe/#flatten_schema","title":"flatten_schema","text":"<p>The <code>flatten_schema</code> can be helpful when dealing with complex (nested) data types. To see it in action, let's define a sample dataframe  <pre><code>df_complex_schema = spark.sql(\n    'SELECT 1 as ID, \"John\" AS Name, struct(\"line1\" AS Line1, \"line2\" AS Line2) AS Address'\n)\ndf_source.printSchema()\n\n\nroot\n |-- ID: integer (nullable = false)\n |-- Name: string (nullable = false)\n |-- Address: struct (nullable = false)\n |    |-- Line1: string (nullable = false)\n |    |-- Line2: string (nullable = false)\n</code></pre> Pass the sample dataframe to get the list of all attributes as  one dimensional list <pre><code>flatten_schema(df_complex_schema.schema)\n</code></pre></p> <p><pre><code>['ID', 'Name', 'Address.Line1', 'Address.Line2']\n</code></pre> Alternatively, the function can return data types of the attributes <pre><code>flatten_schema(\n    schema=df_complex_schema.schema,\n    include_datatype=True\n)\n</code></pre></p> <pre><code>[\n    ('ID', 'IntegerType'),\n    ('Name', 'StringType'),\n    ('Address.Line1', 'StringType'),\n    ('Address.Line2', 'StringType')\n]\n</code></pre>"},{"location":"examples/dataframe/#script_dataframe","title":"script_dataframe","text":"<p>Pass the dataframe to get the <code>script_dataframe</code> to get the script of it that can be ported to another environment. The function can generate the script for dataframes with up to 20 rows. Use <code>.limit(20)</code> to reduce the number of rows when needed</p> <p><pre><code>from spalah.dataframe import script_dataframe\n\nscript = script_dataframe(df)\n\nprint(script)\n</code></pre> output: <pre><code>from pyspark.sql import Row\nimport datetime\nfrom decimal import Decimal\nfrom pyspark.sql.types import *\n\n# Scripted data and schema:\n__data = [Row(ID=1, Name='John', Address=Row(Line1='line1', Line2='line2'))]\n\n__schema = {'type': 'struct', 'fields': [{'name': 'ID', 'type': 'integer', 'nullable': False, 'metadata': {}}, {'name': 'Name', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'Address', 'type': {'type': 'struct', 'fields': [{'name': 'Line1', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'Line2', 'type': 'string', 'nullable': False, 'metadata': {}}]}, 'nullable': False, 'metadata': {}}]}\n\noutcome_dataframe = spark.createDataFrame(__data, StructType.fromJson(__schema))\n</code></pre></p>"},{"location":"examples/dataframe/#schemacomparer","title":"SchemaComparer","text":"<p>Let's define a source and target dataframes that will be used further in the schema comparison. The target schema contains a few adjustments that the class to catch and display <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n\n# A source dataframe\ndf_source = spark.sql(\n    'SELECT 1 as ID, \"John\" AS Name, struct(\"line1\" AS Line1, \"line2\" AS Line2) AS Address'\n)\ndf_source.printSchema()\n\nroot\n |-- ID: integer (nullable = false)\n |-- Name: string (nullable = false)\n |-- Address: struct (nullable = false)\n |    |-- Line1: string (nullable = false)\n |    |-- Line2: string (nullable = false)\n\n\n# A target dataframe\ndf_target = spark.sql(\n    'SELECT \"a\" as ID, \"John\" AS name, struct(\"line1\" AS Line1) AS Address'\n)\ndf_target.printSchema()\n\nroot\n |-- ID: string (nullable = false)             # Changed data type\n |-- name: string (nullable = false)           # Changed case of the column name\n |-- Address: struct (nullable = false)        # Removed field Line2\n |    |-- Line1: string (nullable = false)\n</code></pre></p> <p>Then, let's initiate a SchemaComparer and run the comparison</p> <pre><code>from spalah.dataframe import SchemaComparer\n\nschema_comparer = SchemaComparer(\n    source_schema = df_source.schema,\n    target_schema = df_target.schema\n)\n\nschema_comparer.compare()\n</code></pre> <p>The comparison results are stored in the class instance properties <code>matched</code> and <code>not_matched</code></p> <p><pre><code>schema_comparer.matched\n</code></pre> Contains a single matched column</p> <pre><code>[MatchedColumn(name='Address.Line1',  data_type='StringType')]\n</code></pre> <p><pre><code>schema_comparer.not_matched\n</code></pre> Contains a list of all not matched columns with a reason as description of non-match: <pre><code>[\n    NotMatchedColumn(\n        name='name', \n        data_type='StringType', \n        reason=\"The column exists in source and target schemas but it's name is case-mismatched\"\n    ),\n    NotMatchedColumn(\n        name='ID', \n        data_type='IntegerType &lt;=&gt; StringType', \n        reason='The column exists in source and target schemas but it is not matched by a data type'\n    ),\n    NotMatchedColumn(\n        name='Address.Line2', \n        data_type='StringType', \n        reason='The column exists only in the source schema'\n    )\n]\n</code></pre></p>"},{"location":"examples/dataset/","title":"Examples of use: spalah.dataset","text":"<p>This module contains various storage and dataset specific functions, like: <code>DeltaTableConfig</code>, <code>check_dbfs_mounts</code> etc. </p>"},{"location":"examples/dataset/#deltatableconfig","title":"DeltaTableConfig","text":""},{"location":"examples/dataset/#retrieve-delta-table-properties","title":"Retrieve delta table properties","text":"<pre><code>from spalah.dataset import DeltaTableConfig\n\ndp = DeltaTableConfig(table_path=\"/tmp/nested_schema_dataset\")\n\nprint(dp.properties)\n\n# results to:\n{'delta.deletedFileRetentionDuration': 'interval 15 days'}\n</code></pre>"},{"location":"examples/dataset/#set-delta-table-properties","title":"Set delta table properties","text":"<pre><code>from spalah.dataset import DeltaTableConfig\n\ndp = DeltaTableConfig(table_path=\"/tmp/nested_schema_dataset\")\n\ndp.properties = {\n    \"delta.logRetentionDuration\": \"interval 10 days\",\n    \"delta.deletedFileRetentionDuration\": \"interval 15 days\"\n}\n</code></pre> <p>and the outcome is:</p> <pre><code>2023-05-20 18:21:35,155 INFO      Applying table properties on 'delta.`/tmp/nested_schema_dataset`':\n2023-05-20 18:21:35,156 INFO      Checking if 'delta.logRetentionDuration = interval 10 days' is set on delta.`/tmp/nested_schema_dataset`\n2023-05-20 18:21:35,534 INFO      The property has been set\n2023-05-20 18:21:35,535 INFO      Checking if 'delta.deletedFileRetentionDuration = interval 15 days' is set on delta.`/tmp/nested_schema_dataset`\n2023-05-20 18:21:35,837 INFO      The property has been set\n</code></pre> <p>If the existing properties to be preserved use parameter <code>keep_existing_properties=True</code>:</p> <pre><code>from spalah.dataset import DeltaTableConfig\n\ndp = DeltaTableConfig(table_path=\"/tmp/nested_schema_dataset\")\n\ndp.keep_existing_properties = True\n\ndp.properties = {\n    \"delta.logRetentionDuration\": \"interval 10 days\",\n    \"delta.deletedFileRetentionDuration\": \"interval 15 days\"\n}\n</code></pre>"},{"location":"examples/dataset/#retrieve-delta-table-check-constraints","title":"Retrieve delta table check constraints","text":"<pre><code>from spalah.dataset import DeltaTableConfig\n\ndp = DeltaTableConfig(table_path=\"/tmp/nested_schema_dataset\")\n\nprint(dp.check_constraints)\n\n{} # results to empty dictionary, so no check constraints are set yet\n</code></pre>"},{"location":"examples/dataset/#set-delta-table-check-constraints","title":"Set delta table check constraints","text":"<pre><code>from spalah.dataset import DeltaTableConfig\n\ndp = DeltaTableConfig(table_path=\"/tmp/nested_schema_dataset\")\n\ndp.check_constraints = {'id_is_not_null': 'id is not null'} \n</code></pre> <p>and the outcome is:</p> <pre><code>2023-05-20 18:27:42,070 INFO      Applying check constraints on 'delta.`/tmp/nested_schema_dataset`':\n2023-05-20 18:27:42,071 INFO      Checking if constraint 'id_is_not_null' was already set on delta.`/tmp/nested_schema_dataset`\n2023-05-20 18:27:42,433 INFO      The constraint id_is_not_null has been successfully added to 'delta.`/tmp/nested_schema_dataset`\n</code></pre> <p>If the existing constraints to be preserved use parameter <code>keep_existing_check_constraints=True</code>:</p> <pre><code>from spalah.dataset import DeltaTableConfig\n\ndp = DeltaTableConfig(table_path=\"/tmp/nested_schema_dataset\")\ndp.keep_existing_check_constraints = True\n\ndp.check_constraints = {'Name_is_not_null': 'Name is not null'} \n</code></pre> <p>so, the second check of the check constraints shows both constraints defined: existing and the new one:</p> <pre><code>print(dp.check_constraints)\n\n{'id_is_not_null': 'id is not null', 'name_is_not_null': 'Name is not null'}\n</code></pre> <p>The following check shows that the constraint <code>id_is_not_null</code> is already set and protects the column ID from being null:</p> <pre><code>spark.sql(\n\"\"\"\n    INSERT INTO delta.`/tmp/nested_schema_dataset` (ID, Name, Address)\n    VALUES (NULL, 'Alex', NULL) \n    \"\"\"\n)\n\nERROR Utils: Aborting task\norg.apache.spark.sql.delta.schema.DeltaInvariantViolationException: \nCHECK constraint id_is_not_null (id IS NOT NULL) violated by row with values:\n - id : null\n...\n</code></pre>"},{"location":"reference/dataframe/","title":"spalah.dataframe","text":""},{"location":"reference/dataframe/#spalah.dataframe.SchemaComparer","title":"<code>SchemaComparer</code>","text":"<p>The SchemaComparer is to compare two spark dataframe schemas and find matched and not matched columns.</p> Source code in <code>spalah/dataframe/dataframe.py</code> <pre><code>class SchemaComparer:\n\"\"\"\n    The SchemaComparer is to compare two spark dataframe schemas and find matched\n    and not matched columns.\n    \"\"\"\n\n    def __init__(\n        self, source_schema: T.StringType, target_schema: T.StringType\n    ) -&gt; None:\n\"\"\"Constructs all the necessary input attributes for the SchemaComparer object.\n\n        Args:\n            source_schema (T.StringType): source schema to match\n            target_schema (T.StringType): target schema to match\n\n        Examples:\n            &gt;&gt;&gt; from spalah.dataframe import SchemaComparer\n            &gt;&gt;&gt; schema_comparer = SchemaComparer(\n            ...     source_schema = df_source.schema,\n            ...     target_schema = df_target.schema\n            ... )\n        \"\"\"\n        self._source = self.__import_schema(source_schema)\n        self._target = self.__import_schema(target_schema)\n\n        self.matched: List[tuple] = list()\n\"\"\"List of matched columns\"\"\"\n        self.not_matched: List[tuple] = list()\n\"\"\"The list of not matched columns\"\"\"\n\n    def __import_schema(self, input_schema: T.StructType) -&gt; Set[tuple]:\n\"\"\"Import StructType as the flatten set of tuples: (column_name, data_type)\n\n        Args:\n            input_schema (T.StructType): Schema to process\n\n        Raises:\n            TypeError: if input schema has a type: DataFrame\n            TypeError: if input schema hasn't a type: StructType\n\n        Returns:\n            Set[tuple]: Set of tuples: (column_name, data_type)\n        \"\"\"\n\n        if isinstance(input_schema, DataFrame):\n            raise TypeError(\n                \"One of 'source_schema or 'target_schema' passed as a DataFrame. \"\n                \"Use DataFrame.schema instead\"\n            )\n        elif not isinstance(input_schema, T.StructType):\n            raise TypeError(\n                \"Parameters 'source_schema and 'target_schema' must have a type: StructType\"\n            )\n\n        return set(flatten_schema(input_schema, True))\n\n    def __match_by_name_and_type(\n        self, source: Set[tuple] = set(), target: Set[tuple] = set()\n    ) -&gt; Set[tuple]:\n\"\"\"Matches columns in source and target schemas by name and data type\n\n        Args:\n            source (Set[tuple], optional): Flattened source schema. Defaults to set().\n            target (Set[tuple], optional): Flattened target schema. Defaults to set().\n\n        Returns:\n            Set[tuple]: Fully matched columns as a set of tuples: (column_name, data_type)\n        \"\"\"\n\n        # If source and target is not provided, use class attributes as the input\n        _source = self._source if not source else source\n        _target = self._target if not target else target\n\n        result = _source &amp; _target\n\n        # Remove matched values of case 1 from further processing\n        self.__remove_matched_by_name_and_type(result)\n\n        if not (source and target):\n            self.__populate_matched(result)\n\n        return result\n\n    def __remove_matched_by_name_and_type(self, subtract_value: Set[tuple]) -&gt; None:\n\"\"\"Removes fully matched columns from the further processing\n\n        Args:\n            subtract_value (Set[tuple]): Set of matched columns\n        \"\"\"\n\n        self._source = self._source - subtract_value\n        self._target = self._target - subtract_value\n\n    def __remove_matched_by_name(self, subtract_value: Set[tuple]) -&gt; None:\n\"\"\"Removes matched by name columns from the further processing\n\n        Args:\n            subtract_value (Set[tuple]): Set of matched column\n        \"\"\"\n\n        def _remove(input_value: Set[tuple], subtract_value: Set[tuple]) -&gt; Set[tuple]:\n\"\"\"Internal helper for removal of Tuples by the first member\"\"\"\n            return {\n                (x, y)\n                for (x, y) in input_value\n                if x.lower() not in [z[0].lower() for z in subtract_value]\n            }\n\n        self._source = _remove(self._source, subtract_value)  # type: ignore\n        self._target = _remove(self._target, subtract_value)  # type: ignore\n\n    def __lower_column_names(self, base_value: Set[tuple]) -&gt; Set[tuple]:\n\"\"\"Lower-case all column names of the input set\n\n        Args:\n            base_value (Set[tuple]): Input set of columns\n\n        Returns:\n            Set[tuple]: Output set of columns with lower-case column names\n        \"\"\"\n        return {(x.lower(), y) for (x, y) in base_value}\n\n    def __match_by_name_type_excluding_case(self) -&gt; None:\n\"\"\"Matches columns in source and target schemas by name and data type\n        without taking into account column name case\n        \"\"\"\n\n        _source_lowered = self.__lower_column_names(self._source)\n        _target_lowered = self.__lower_column_names(self._target)\n\n        result = self.__match_by_name_and_type(_source_lowered, _target_lowered)\n\n        # Remove matched values of case 2 from further processing\n        self.__remove_matched_by_name(result)\n\n        self.__populate_not_matched(\n            result,\n            \"The column exists in source and target schemas but it's name is case-mismatched\",\n        )\n\n    def __match_by_name_but_not_type(self) -&gt; None:\n\"\"\"Matches columns in source and target schemas only by column name\"\"\"\n\n        x = dict(self._source)  # type: ignore\n        y = dict(self._target)  # type: ignore\n\n        result = {(k, f\"{x[k]} &lt;=&gt; {y[k]}\") for k in x if k in y and x[k] != y[k]}\n\n        # Remove matched values of case 3 from further processing\n        self.__remove_matched_by_name(result)  # type: ignore\n\n        self.__populate_not_matched(\n            result,  # type: ignore\n            \"The column exists in source and target schemas but it is not matched by a data type\",\n        )\n\n    def __process_remaining_non_matched_columns(self) -&gt; None:\n\"\"\"Process remaining not matched columns\"\"\"\n\n        self.__populate_not_matched(\n            self._source, \"The column exists only in the source schema\"\n        )\n\n        self.__populate_not_matched(\n            self._target, \"The column exists only in the target schema\"\n        )\n\n        self.__remove_matched_by_name(self._source)\n        self.__remove_matched_by_name(self._target)\n\n    def __populate_matched(self, input_value: Set[tuple]) -&gt; None:\n\"\"\"Populate class property 'matched' with a list of fully matched columns\n\n        Args:\n            input_value (Set[tuple]): The set of tuples with a list of column names and data types\n        \"\"\"\n\n        for match in input_value:\n            self.matched.append(MatchedColumn(name=match[0], data_type=match[1]))\n\n    def __populate_not_matched(self, input_value: Set[tuple], reason: str) -&gt; None:\n\"\"\"Populate class property 'not_matched' with a list of columns that didn't match for some\n        reason with included an actual reason\n\n        Args:\n            input_value (Set[tuple]): The set of tuples with a list of column names and data types\n            reason (str): Reason for not match\n\n        \"\"\"\n\n        for match in input_value:\n            self.not_matched.append(\n                NotMatchedColumn(name=match[0], data_type=match[1], reason=reason)\n            )\n\n    def compare(self) -&gt; None:\n\"\"\"\n        Compares the source and target schemas and populates properties `matched` and `not_matched`\n\n        Examples:\n            &gt;&gt;&gt; # instantiate schema_comparer firstly, see example above\n            &gt;&gt;&gt; schema_comparer.compare()\n\n            Get list of all columns that are matched by name and type:\n            &gt;&gt;&gt; schema_comparer.matched\n            [MatchedColumn(name='Address.Line1',  data_type='StringType')]\n\n            Get unmatched columns:\n            &gt;&gt;&gt; schema_comparer.not_matched\n            [\n                NotMatchedColumn(\n                    name='name',\n                    data_type='StringType',\n                    reason=\"The column exists in source and target schemas but it's name is \\\ncase-mismatched\"\n                ),\n                NotMatchedColumn(\n                    name='Address.Line2',\n                    data_type='StringType',\n                    reason='The column exists only in the source schema'\n                )\n            ]\n        \"\"\"\n\n        # Case 1: find columns that are matched by name and type and remove them\n        # from further processing\n        self.__match_by_name_and_type()\n\n        # Case 2: find columns that match mismatched by name due to case: ID &lt;-&gt; Id\n        self.__match_by_name_type_excluding_case()\n\n        # Case 3: Find columns matched by name, but not by data type\n        self.__match_by_name_but_not_type()\n\n        # Case 4: Find columns that exists only in the source or target\n        self.__process_remaining_non_matched_columns()\n</code></pre>"},{"location":"reference/dataframe/#spalah.dataframe.dataframe.SchemaComparer.matched","title":"<code>matched: List[tuple] = list()</code>  <code>instance-attribute</code>","text":"<p>List of matched columns</p>"},{"location":"reference/dataframe/#spalah.dataframe.dataframe.SchemaComparer.not_matched","title":"<code>not_matched: List[tuple] = list()</code>  <code>instance-attribute</code>","text":"<p>The list of not matched columns</p>"},{"location":"reference/dataframe/#spalah.dataframe.dataframe.SchemaComparer.__init__","title":"<code>__init__(source_schema, target_schema)</code>","text":"<p>Constructs all the necessary input attributes for the SchemaComparer object.</p> <p>Parameters:</p> Name Type Description Default <code>source_schema</code> <code>T.StringType</code> <p>source schema to match</p> required <code>target_schema</code> <code>T.StringType</code> <p>target schema to match</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spalah.dataframe import SchemaComparer\n&gt;&gt;&gt; schema_comparer = SchemaComparer(\n...     source_schema = df_source.schema,\n...     target_schema = df_target.schema\n... )\n</code></pre> Source code in <code>spalah/dataframe/dataframe.py</code> <pre><code>def __init__(\n    self, source_schema: T.StringType, target_schema: T.StringType\n) -&gt; None:\n\"\"\"Constructs all the necessary input attributes for the SchemaComparer object.\n\n    Args:\n        source_schema (T.StringType): source schema to match\n        target_schema (T.StringType): target schema to match\n\n    Examples:\n        &gt;&gt;&gt; from spalah.dataframe import SchemaComparer\n        &gt;&gt;&gt; schema_comparer = SchemaComparer(\n        ...     source_schema = df_source.schema,\n        ...     target_schema = df_target.schema\n        ... )\n    \"\"\"\n    self._source = self.__import_schema(source_schema)\n    self._target = self.__import_schema(target_schema)\n\n    self.matched: List[tuple] = list()\n\"\"\"List of matched columns\"\"\"\n    self.not_matched: List[tuple] = list()\n\"\"\"The list of not matched columns\"\"\"\n</code></pre>"},{"location":"reference/dataframe/#spalah.dataframe.dataframe.SchemaComparer.compare","title":"<code>compare()</code>","text":"<p>Compares the source and target schemas and populates properties <code>matched</code> and <code>not_matched</code></p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # instantiate schema_comparer firstly, see example above\n&gt;&gt;&gt; schema_comparer.compare()\n</code></pre> <p>Get list of all columns that are matched by name and type:</p> <pre><code>&gt;&gt;&gt; schema_comparer.matched\n[MatchedColumn(name='Address.Line1',  data_type='StringType')]\n</code></pre> <p>Get unmatched columns:</p> <pre><code>&gt;&gt;&gt; schema_comparer.not_matched\n[\n    NotMatchedColumn(\n        name='name',\n        data_type='StringType',\n        reason=\"The column exists in source and target schemas but it's name is case-mismatched\"\n    ),\n    NotMatchedColumn(\n        name='Address.Line2',\n        data_type='StringType',\n        reason='The column exists only in the source schema'\n    )\n]\n</code></pre> Source code in <code>spalah/dataframe/dataframe.py</code> <pre><code>    def compare(self) -&gt; None:\n\"\"\"\n        Compares the source and target schemas and populates properties `matched` and `not_matched`\n\n        Examples:\n            &gt;&gt;&gt; # instantiate schema_comparer firstly, see example above\n            &gt;&gt;&gt; schema_comparer.compare()\n\n            Get list of all columns that are matched by name and type:\n            &gt;&gt;&gt; schema_comparer.matched\n            [MatchedColumn(name='Address.Line1',  data_type='StringType')]\n\n            Get unmatched columns:\n            &gt;&gt;&gt; schema_comparer.not_matched\n            [\n                NotMatchedColumn(\n                    name='name',\n                    data_type='StringType',\n                    reason=\"The column exists in source and target schemas but it's name is \\\ncase-mismatched\"\n                ),\n                NotMatchedColumn(\n                    name='Address.Line2',\n                    data_type='StringType',\n                    reason='The column exists only in the source schema'\n                )\n            ]\n        \"\"\"\n\n        # Case 1: find columns that are matched by name and type and remove them\n        # from further processing\n        self.__match_by_name_and_type()\n\n        # Case 2: find columns that match mismatched by name due to case: ID &lt;-&gt; Id\n        self.__match_by_name_type_excluding_case()\n\n        # Case 3: Find columns matched by name, but not by data type\n        self.__match_by_name_but_not_type()\n\n        # Case 4: Find columns that exists only in the source or target\n        self.__process_remaining_non_matched_columns()\n</code></pre>"},{"location":"reference/dataframe/#spalah.dataframe.flatten_schema","title":"<code>flatten_schema(schema, include_datatype=False, column_prefix=None)</code>","text":"<p>Parses spark dataframe schema and returns the list of columns If the schema is nested, the columns are flattened</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>StructType</code> <p>Input dataframe schema</p> required <code>include_type</code> <code>bool</code> <p>Flag to include column types</p> required <code>column_prefix</code> <code>str</code> <p>Column name prefix. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>The list of (flattened) column names</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spalah.dataframe import flatten_schema\n&gt;&gt;&gt; flatten_schema(schema=df_complex_schema.schema)\n</code></pre> <p>returns the list of columns, nested are flattened:</p> <pre><code>&gt;&gt;&gt; ['ID', 'Name', 'Address.Line1', 'Address.Line2']\n</code></pre> Source code in <code>spalah/dataframe/dataframe.py</code> <pre><code>def flatten_schema(\n    schema: T.StructType,\n    include_datatype: bool = False,\n    column_prefix: Optional[str] = None,\n) -&gt; list:\n\"\"\"Parses spark dataframe schema and returns the list of columns\n    If the schema is nested, the columns are flattened\n\n    Args:\n        schema (StructType): Input dataframe schema\n        include_type (bool, optional): Flag to include column types\n        column_prefix (str, optional): Column name prefix. Defaults to None.\n\n    Returns:\n        The list of (flattened) column names\n\n    Examples:\n        &gt;&gt;&gt; from spalah.dataframe import flatten_schema\n        &gt;&gt;&gt; flatten_schema(schema=df_complex_schema.schema)\n\n        returns the list of columns, nested are flattened:\n        &gt;&gt;&gt; ['ID', 'Name', 'Address.Line1', 'Address.Line2']\n    \"\"\"\n\n    if not isinstance(schema, T.StructType):\n        raise TypeError(\"Parameter schema must be a StructType\")\n\n    columns = []\n\n    for column in schema.fields:\n        if column_prefix:\n            name = column_prefix + \".\" + column.name\n        else:\n            name = column.name\n\n        column_data_type = column.dataType\n\n        if isinstance(column_data_type, T.ArrayType):\n            column_data_type = column_data_type.elementType\n\n        if isinstance(column_data_type, T.StructType):\n            columns += flatten_schema(\n                column_data_type,\n                include_datatype=include_datatype,\n                column_prefix=name,\n            )\n        else:\n            if include_datatype:\n                result = name, str(column_data_type)\n            else:\n                result = name\n\n            columns.append(result)\n\n    return columns\n</code></pre>"},{"location":"reference/dataframe/#spalah.dataframe.script_dataframe","title":"<code>script_dataframe(input_dataframe, suppress_print_output=True)</code>","text":"<p>Generate a script to recreate the dataframe The script includes the schema and the data</p> <p>Parameters:</p> Name Type Description Default <code>input_dataframe</code> <code>DataFrame</code> <p>Input spark dataframe</p> required <code>suppress_print_output</code> <code>bool</code> <p>Disable prints to console.             Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>when the dataframe is too large (by default &gt; 20 rows)</p> <p>Returns:</p> Type Description <code>str</code> <p>The script to recreate the dataframe</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spalah.dataframe import script_dataframe\n&gt;&gt;&gt; script = script_dataframe(input_dataframe=df)\n&gt;&gt;&gt; print(script)\n</code></pre> Source code in <code>spalah/dataframe/dataframe.py</code> <pre><code>def script_dataframe(\n    input_dataframe: DataFrame, suppress_print_output: bool = True\n) -&gt; str:\n\"\"\"Generate a script to recreate the dataframe\n    The script includes the schema and the data\n\n    Args:\n        input_dataframe (DataFrame): Input spark dataframe\n        suppress_print_output (bool, optional): Disable prints to console. \\\n            Defaults to True.\n\n    Raises:\n        ValueError: when the dataframe is too large (by default &gt; 20 rows)\n\n    Returns:\n        The script to recreate the dataframe\n\n    Examples:\n        &gt;&gt;&gt; from spalah.dataframe import script_dataframe\n        &gt;&gt;&gt; script = script_dataframe(input_dataframe=df)\n        &gt;&gt;&gt; print(script)\n    \"\"\"\n\n    MAX_ROWS_IN_SCRIPT = 20\n\n    __dataframe = input_dataframe\n\n    if __dataframe.count() &gt; MAX_ROWS_IN_SCRIPT:\n        raise ValueError(\n            \"This method is limited to script up \"\n            f\"to {MAX_ROWS_IN_SCRIPT} row(s) per call\"\n        )\n\n    __schema = input_dataframe.schema.jsonValue()\n\n    __script_lines = [\n        \"from pyspark.sql import Row\",\n        \"import datetime\",\n        \"from decimal import Decimal\",\n        \"from pyspark.sql.types import *\",\n        \"\",\n        \"# Scripted data and schema:\",\n        f\"__data = {pformat(__dataframe.collect())}\",\n        \"\",\n        f\"__schema = {__schema}\",\n        \"\",\n        \"outcome_dataframe = spark.createDataFrame(__data, StructType.fromJson(__schema))\",\n    ]\n\n    __final_script = \"\\n\".join(__script_lines)\n\n    if not suppress_print_output:\n        print(\"#\", \"=\" * 80)\n        print(\n            \"# IMPORTANT!!! REMOVE PII DATA BEFORE RE-CREATING IT IN NON-PRODUCTION ENVIRONMENTS\",\n            \" \" * 3,\n            \"#\",\n        )\n        print(\"#\", \"=\" * 80)\n        print(\"\")\n        print(__final_script)\n\n    return __final_script\n</code></pre>"},{"location":"reference/dataframe/#spalah.dataframe.slice_dataframe","title":"<code>slice_dataframe(input_dataframe, columns_to_include=None, columns_to_exclude=None, nullify_only=False, generate_sql=False, debug=False)</code>","text":"<p>Process flat or nested schema of the dataframe by slicing the schema or nullifying columns</p> <p>Parameters:</p> Name Type Description Default <code>input_dataframe</code> <code>DataFrame</code> <p>Input dataframe</p> required <code>columns_to_include</code> <code>Optional[List]</code> <p>Columns that must remain in the dataframe unchanged</p> <code>None</code> <code>columns_to_exclude</code> <code>Optional[List]</code> <p>Columns that must be removed (or nullified)</p> <code>None</code> <code>nullify_only</code> <code>bool</code> <p>Nullify columns instead of removing them. Defaults to False</p> <code>False</code> <code>debug</code> <code>bool</code> <p>For extra debug output. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the 'column_to_include' or 'column_to_exclude' are not type list</p> <code>ValueError</code> <p>If the included columns overlay excluded columns, so nothing to return</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The processed dataframe</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spalah.dataframe import slice_dataframe\n&gt;&gt;&gt; df = spark.sql(\n...         'SELECT 1 as ID, \"John\" AS Name,\n...         struct(\"line1\" AS Line1, \"line2\" AS Line2) AS Address'\n...     )\n&gt;&gt;&gt; df_sliced = slice_dataframe(\n...     input_dataframe=df,\n...     columns_to_include=[\"Name\", \"Address\"],\n...     columns_to_exclude=[\"Address.Line2\"]\n... )\n</code></pre> <p>As the result, the dataframe will contain only the columns <code>Name</code> and <code>Address.Line1</code>             because <code>Name</code> and <code>Address</code> are included and a nested element <code>Address.Line2</code> is             excluded</p> <pre><code>&gt;&gt;&gt; df_result.printSchema()\nroot\n|-- Name: string (nullable = false)\n|-- Address: struct (nullable = false)\n|    |-- Line1: string (nullable = false)\n</code></pre> Source code in <code>spalah/dataframe/dataframe.py</code> <pre><code>def slice_dataframe(\n    input_dataframe: DataFrame,\n    columns_to_include: Optional[List] = None,\n    columns_to_exclude: Optional[List] = None,\n    nullify_only: bool = False,\n    generate_sql: bool = False,\n    debug: bool = False,\n) -&gt; DataFrame:\n\"\"\"Process flat or nested schema of the dataframe by slicing the schema\n    or nullifying columns\n\n    Args:\n        input_dataframe (DataFrame): Input dataframe\n        columns_to_include (Optional[List]): Columns that must remain in the dataframe unchanged\n        columns_to_exclude (Optional[List]): Columns that must be removed (or nullified)\n        nullify_only (bool, optional): Nullify columns instead of removing them. Defaults to False\n        debug (bool, optional): For extra debug output. Defaults to False.\n\n    Raises:\n        TypeError: If the 'column_to_include' or 'column_to_exclude' are not type list\n        ValueError: If the included columns overlay excluded columns, so nothing to return\n\n    Returns:\n        DataFrame: The processed dataframe\n\n    Examples:\n        &gt;&gt;&gt; from spalah.dataframe import slice_dataframe\n        &gt;&gt;&gt; df = spark.sql(\n        ...         'SELECT 1 as ID, \"John\" AS Name,\n        ...         struct(\"line1\" AS Line1, \"line2\" AS Line2) AS Address'\n        ...     )\n        &gt;&gt;&gt; df_sliced = slice_dataframe(\n        ...     input_dataframe=df,\n        ...     columns_to_include=[\"Name\", \"Address\"],\n        ...     columns_to_exclude=[\"Address.Line2\"]\n        ... )\n\n        As the result, the dataframe will contain only the columns `Name` and `Address.Line1` \\\n            because `Name` and `Address` are included and a nested element `Address.Line2` is \\\n            excluded\n        &gt;&gt;&gt; df_result.printSchema()\n        root\n        |-- Name: string (nullable = false)\n        |-- Address: struct (nullable = false)\n        |    |-- Line1: string (nullable = false)\n    \"\"\"\n\n    projection = []\n    spark = SparkSession.getActiveSession()\n\n    # Verification of input parameters:\n\n    if input_dataframe and not isinstance(input_dataframe, DataFrame):\n        raise TypeError(\"input_dataframe must be a dataframe\")\n    elif input_dataframe and isinstance(input_dataframe, DataFrame):\n        _schema = input_dataframe.schema\n        _table_identifier = \"input_dataframe\"\n        _input_dataframe = input_dataframe\n\n    if not columns_to_include:\n        columns_to_include = []\n\n    if not columns_to_exclude:\n        columns_to_exclude = []\n\n    if not (type(columns_to_include) is list and type(columns_to_exclude) is list):\n        raise TypeError(\n            \"The type of parameters 'columns_to_include', 'columns_to_exclude' \"\n            \"must be a list\"\n        )\n\n    if not all(\n        isinstance(item, str) for item in columns_to_include + columns_to_exclude\n    ):\n        raise TypeError(\n            \"Members of 'columns_to_include' and 'columns_to_exclude' \"\n            \"must be a string\"\n        )\n\n    if debug:\n        print(\"The list of columns to include:\")\n        pprint(columns_to_include)\n\n        print(\"The list of columns to exclude:\")\n        pprint(columns_to_exclude)\n\n        if nullify_only:\n            print(\"Columns to nullify in the final projection:\")\n        else:\n            print(\"Columns to include into the final projection:\")\n\n    # lower-case items for making further filtering case-insensitive\n    columns_to_include = [item.lower() for item in columns_to_include]\n    columns_to_exclude = [item.lower() for item in columns_to_exclude]\n\n    for field in _schema.fields:\n        node_result = __process_schema_node(\n            node=field,\n            columns_to_include=columns_to_include,\n            columns_to_exclude=columns_to_exclude,\n            nullify_only=nullify_only,\n            debug=debug,\n        )\n\n        if node_result is not None:\n            projection.append(node_result)\n\n    if not projection:\n        raise ValueError(\n            \"At least one column should be listed in the \"\n            + \"columns_to_include/columns_to_exclude attributes \"\n            + \"and included column should not directly overlap with excluded one\"\n        )\n\n    if generate_sql:\n        delimiter = \", \\n\"\n        result = f\"SELECT \\n{delimiter.join( projection)} \\nFROM {_table_identifier}\"\n    else:\n        result = _input_dataframe.selectExpr(*projection)\n\n    return result\n</code></pre>"},{"location":"reference/dataset/DeltaTableConfig/","title":"spalah.dataset.DeltaTableConfig","text":""},{"location":"reference/dataset/DeltaTableConfig/#spalah.dataset.DeltaTableConfig.DeltaTableConfig","title":"<code>spalah.dataset.DeltaTableConfig.DeltaTableConfig(table_path='', table_name='', spark_session=None)</code>","text":"<p>Manages Delta Table properties, constraints, etc.</p> <p>Attributes:</p> Name Type Description <code>keep_existing_properties</code> <code>bool</code> <p>Preserves existing table properties if they are not                                     in the input value. Defaults to False</p> <code>keep_existing_check_constraints</code> <code>bool</code> <p>Preserves existing table constraints if they are not                                     in the input value. Defaults to False</p> <p>Parameters:</p> Name Type Description Default <code>table_path</code> <code>str</code> <p>Path to delta table. For instance: /mnt/db1/table1</p> <code>''</code> <code>table_name</code> <code>str</code> <p>Delta table name. For instance: db1.table1</p> <code>''</code> <code>spark_session</code> <code>SparkSession</code> <p>(SparkSession, optional)  The current spark context.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if values for both 'table_path' and 'table_name' provided         provide values to one of them</p> <code>ValueError</code> <p>if values for neither 'table_path' nor 'table_name' provided         provide values to one of them</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spalah.datalake import DeltaTableConfig\n&gt;&gt;&gt; dp = DeltaTableConfig(table_path=\"/path/dataset\")\n&gt;&gt;&gt; print(dp.properties)\n{'delta.deletedFileRetentionDuration': 'interval 15 days'}\n</code></pre> Source code in <code>spalah/dataset/DeltaTableConfig.py</code> <pre><code>def __init__(\n    self,\n    table_path: str = \"\",\n    table_name: str = \"\",\n    spark_session: SparkSession = None,\n) -&gt; None:\n\"\"\"\n    Args:\n        table_path (str, optional): Path to delta table. For instance: /mnt/db1/table1\n        table_name (str, optional): Delta table name. For instance: db1.table1\n        spark_session: (SparkSession, optional)  The current spark context.\n    Raises:\n        ValueError: if values for both 'table_path' and 'table_name' provided\n                    provide values to one of them\n        ValueError: if values for neither 'table_path' nor 'table_name' provided\n                    provide values to one of them\n    Examples:\n        &gt;&gt;&gt; from spalah.datalake import DeltaTableConfig\n        &gt;&gt;&gt; dp = DeltaTableConfig(table_path=\"/path/dataset\")\n        &gt;&gt;&gt; print(dp.properties)\n        {'delta.deletedFileRetentionDuration': 'interval 15 days'}\n    \"\"\"\n\n    self.spark_session = (\n        SparkSession.getActiveSession() if not spark_session else spark_session\n    )\n    self.table_name = self.__get_table_identifier(\n        table_path=table_path, table_name=table_name\n    )\n    self.original_table_name = table_name\n</code></pre>"},{"location":"reference/dataset/DeltaTableConfig/#spalah.dataset.DeltaTableConfig.DeltaTableConfig.check_constraints","title":"<code>check_constraints: Union[dict, None]</code>  <code>property</code> <code>writable</code>","text":"<p>Gets/sets dataset's delta table check constraints.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>dict</code> <p>An input dictionary in the format: <code>{\"property_name\": \"value\"}</code></p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spalah.datalake import DeltaTableConfig\n&gt;&gt;&gt; dp = DeltaTableConfig(table_path=\"/path/dataset\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # get existing constraints\n&gt;&gt;&gt; print(dp.check_constraints)\n{}\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Add a new check constraint\n&gt;&gt;&gt; dp.check_constraints = {'id_is_not_null': 'id is not null'}\n</code></pre>"},{"location":"reference/dataset/DeltaTableConfig/#spalah.dataset.DeltaTableConfig.DeltaTableConfig.properties","title":"<code>properties: Union[dict, None]</code>  <code>property</code> <code>writable</code>","text":"<p>Gets/sets dataset's delta table properties.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>dict</code> <p>An input dictionary in the format: <code>{\"property_name\": \"value\"}</code></p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spalah.datalake import DeltaTableConfig\n&gt;&gt;&gt; dp = DeltaTableConfig(table_path=\"/path/dataset\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # get existing properties\n&gt;&gt;&gt; print(dp.properties)\n{'delta.deletedFileRetentionDuration': 'interval 15 days'}\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Adjust the property value from 15 to 30 days\n&gt;&gt;&gt; dp.properties = {'delta.deletedFileRetentionDuration': 'interval 30 days'}\n</code></pre>"},{"location":"reference/dataset/dbfs/","title":"spalah.dataset.dbfs","text":""},{"location":"reference/dataset/dbfs/#spalah.dataset.dbfs","title":"<code>spalah.dataset.dbfs</code>","text":""},{"location":"reference/dataset/dbfs/#spalah.dataset.dbfs.check_dbfs_mounts","title":"<code>check_dbfs_mounts(mounts, print_output=False)</code>","text":"<p>DBFS Helper: checks if dbfs mounts are accessible</p> <p>Parameters:</p> Name Type Description Default <code>mounts</code> <code>list</code> <p>The list of mounts from <code>dbutils.fs.mounts()</code></p> required <code>print_output</code> <code>bool</code> <p>Flag to print debug information. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>the list of mounts with corresponding statuses</p> Source code in <code>spalah/dataset/dbfs.py</code> <pre><code>def check_dbfs_mounts(mounts: list, print_output: bool = False) -&gt; list:\n\"\"\"DBFS Helper: checks if dbfs mounts are accessible\n\n    Args:\n        mounts (list): The list of mounts from `dbutils.fs.mounts()`\n        print_output (bool, optional): Flag to print debug information. Defaults to False.\n\n    Returns:\n        list: the list of mounts with corresponding statuses\n    \"\"\"\n\n    dbfs_mount = namedtuple(\"dbfs_mount\", (\"result\", \"mount\"))\n    _result = list()\n\n    for mount in mounts:\n        _mount_path = f\"/dbfs{mount.mountPoint}\"\n\n    if (\n        \"/databricks\" not in _mount_path\n        and os.path.isdir(_mount_path)\n        and \"mount.err\" not in os.listdir(_mount_path)\n    ):\n        _result.append(dbfs_mount(\"ok\", mount.mountPoint))\n    else:\n        _result.append(dbfs_mount(\"failed\", mount.mountPoint))\n\n    if print_output:\n        for item in _result:\n            print(f\"{item.result:&lt;8}  {item.mount}\")\n\n    return _result\n</code></pre>"}]}