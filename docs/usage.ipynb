{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from spalah.dataframe import slice_dataframe\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/08/05 12:32:11 WARN Utils: Your hostname, Alexandrs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.68.121 instead (on interface en0)\n",
      "22/08/05 12:32:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/alexandrvolok/repos/.envs/psdt/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/08/05 12:32:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SchemaComparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = false)\n",
      " |-- Name: string (nullable = false)\n",
      " |-- Address: struct (nullable = false)\n",
      " |    |-- Line1: string (nullable = false)\n",
      " |    |-- Line2: string (nullable = false)\n",
      "\n",
      "root\n",
      " |-- ID: string (nullable = false)\n",
      " |-- name: string (nullable = false)\n",
      " |-- Address: struct (nullable = false)\n",
      " |    |-- Line1: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_source = spark.sql(\n",
    "    'SELECT 1 as ID, \"John\" AS Name, struct(\"line1\" AS Line1, \"line2\" AS Line2) AS Address'\n",
    ")\n",
    "df_source.printSchema()\n",
    "\n",
    "df_target = spark.sql(\n",
    "    'SELECT \"a\" as ID, \"John\" AS name, struct(\"line1\" AS Line1) AS Address'\n",
    ")\n",
    "df_target.printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NotMatchedColumn(name='name', data_type='StringType', reason=\"The column exists in source and target schemas but it's name is case-mismatched\"),\n",
       " NotMatchedColumn(name='ID', data_type='IntegerType <=> StringType', reason='The column exists in source and target schemas but it is not matched by a data type'),\n",
       " NotMatchedColumn(name='Address.Line2', data_type='StringType', reason='The column exists only in the source schema')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_comparer = SchemaComparer(\n",
    "    source_schema = df_source.schema,\n",
    "    target_schema = df_target.schema\n",
    ")\n",
    "\n",
    "schema_comparer.compare()\n",
    "\n",
    "schema_comparer.matched\n",
    "schema_comparer.not_matched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatten_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spalah.dataframe import flatten_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = false)\n",
      " |-- Name: string (nullable = false)\n",
      " |-- Address: struct (nullable = false)\n",
      " |    |-- Line1: string (nullable = false)\n",
      " |    |-- Line2: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_complex_schema = spark.sql(\n",
    "    'SELECT 1 as ID, \"John\" AS Name, struct(\"line1\" AS Line1, \"line2\" AS Line2) AS Address'\n",
    ")\n",
    "df_source.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ID', 'IntegerType'),\n",
       " ('Name', 'StringType'),\n",
       " ('Address.Line1', 'StringType'),\n",
       " ('Address.Line2', 'StringType')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_schema(\n",
    "    schema=df_complex_schema.schema,\n",
    "    include_datatype=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = false)\n",
      " |-- Name: string (nullable = false)\n",
      " |-- Address: struct (nullable = false)\n",
      " |    |-- Line1: string (nullable = false)\n",
      " |    |-- Line2: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\n",
    "    'SELECT 1 as ID, \"John\" AS Name, struct(\"line1\" AS Line1, \"line2\" AS Line2) AS Address'\n",
    ")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from pyspark.sql import Row\n",
      "import datetime\n",
      "from decimal import Decimal\n",
      "from pyspark.sql.types import *\n",
      "\n",
      "# Scripted data and schema:\n",
      "__data = [Row(ID=1, Name='John', Address=Row(Line1='line1', Line2='line2'))]\n",
      "\n",
      "__schema = {'type': 'struct', 'fields': [{'name': 'ID', 'type': 'integer', 'nullable': False, 'metadata': {}}, {'name': 'Name', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'Address', 'type': {'type': 'struct', 'fields': [{'name': 'Line1', 'type': 'string', 'nullable': False, 'metadata': {}}, {'name': 'Line2', 'type': 'string', 'nullable': False, 'metadata': {}}]}, 'nullable': False, 'metadata': {}}]}\n",
      "\n",
      "outcome_dataframe = spark.createDataFrame(__data, StructType.fromJson(__schema))\n"
     ]
    }
   ],
   "source": [
    "from spalah.dataframe import script_dataframe\n",
    "\n",
    "script = script_dataframe(df)\n",
    "\n",
    "print(script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slice_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = false)\n",
      " |-- Name: string (nullable = false)\n",
      " |-- Address: struct (nullable = false)\n",
      " |    |-- Line1: string (nullable = false)\n",
      " |    |-- Line2: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spalah.dataframe import slice_dataframe\n",
    "\n",
    "df = spark.sql(\n",
    "    'SELECT 1 as ID, \"John\" AS Name, struct(\"line1\" AS Line1, \"line2\" AS Line2) AS Address'\n",
    ")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = false)\n",
      " |-- Address: struct (nullable = false)\n",
      " |    |-- Line1: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result = slice_dataframe(\n",
    "    input_dataframe=df,\n",
    "    columns_to_include=[\"Name\", \"Address\"],\n",
    "    columns_to_exclude=[\"Address.Line2\"]\n",
    ")\n",
    "\n",
    "df_result.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------------+\n",
      "|  ID|Name|      Address|\n",
      "+----+----+-------------+\n",
      "|null|John|{line1, null}|\n",
      "+----+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alternatively, excluded columns can be nullified instead of removed\n",
    "df_result = slice_dataframe(\n",
    "    input_dataframe=df,\n",
    "    columns_to_include=[\"Name\", \"Address\"],\n",
    "    columns_to_exclude=[\"Address.Line2\"],\n",
    "    nullify_only=True\n",
    ")\n",
    "\n",
    "df_result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('spalah')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "257711548178d2cb2d5540aed74584fc22fa35e89db7e47a5d0185909a4181b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
